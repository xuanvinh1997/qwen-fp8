{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "class SparseRouterMoE(nn.Module):\n",
    "    \"\"\"\n",
    "    Sparse router for Mixture of Experts that decides which expert to use for each input.\n",
    "    Uses a k-sparse activation where only top-k experts are selected.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, num_experts, k=1):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.num_experts = num_experts\n",
    "        self.k = k  # Number of experts to route to\n",
    "        \n",
    "        # Router network (maps input to expert selection probabilities)\n",
    "        self.router = nn.Linear(input_size, num_experts)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Input embedding or hidden state\n",
    "        Returns:\n",
    "          - dispatch_tensor: Binary tensor indicating which experts to use\n",
    "          - combine_tensor: Weights for combining expert outputs\n",
    "          - router_logits: Raw logits from router\n",
    "        \"\"\"\n",
    "        # Get router logits and probabilities\n",
    "        router_logits = self.router(x)  # [batch_size, seq_len, num_experts]\n",
    "        \n",
    "        # Apply softmax to get expert selection probabilities\n",
    "        router_probs = F.softmax(router_logits, dim=-1)\n",
    "        \n",
    "        # Select top-k experts\n",
    "        top_k_probs, top_k_indices = torch.topk(router_probs, self.k, dim=-1)\n",
    "        \n",
    "        # Normalize the top-k probabilities\n",
    "        top_k_probs_sum = torch.sum(top_k_probs, dim=-1, keepdim=True)\n",
    "        top_k_probs = top_k_probs / top_k_probs_sum\n",
    "        \n",
    "        # Create dispatch tensor (indicates which experts to use)\n",
    "        dispatch_tensor = torch.zeros_like(router_probs)\n",
    "        \n",
    "        # Use scatter to place top-k probabilities in the dispatch tensor\n",
    "        dispatch_tensor.scatter_(-1, top_k_indices, top_k_probs)\n",
    "        \n",
    "        return dispatch_tensor, top_k_probs, top_k_indices, router_logits\n",
    "\n",
    "class MoEInput:\n",
    "    \"\"\"Helper class to store input representation for routing\"\"\"\n",
    "    def __init__(self, input_ids, attention_mask=None):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "\n",
    "class QwenMoEEnsemble(nn.Module):\n",
    "    \"\"\"\n",
    "    Ensemble model using Sparse Mixture of Experts architecture with \n",
    "    Qwen models as experts.\n",
    "    \"\"\"\n",
    "    def __init__(self, expert_models, device=\"cuda\", router_dim=768, k=1):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.num_experts = len(expert_models)\n",
    "        self.k = k\n",
    "        \n",
    "        # Load expert models and tokenizers\n",
    "        self.experts = []\n",
    "        self.tokenizers = []\n",
    "        \n",
    "        print(f\"Loading {self.num_experts} expert models...\")\n",
    "        for model_info in expert_models:\n",
    "            model_name = model_info[\"source_model\"]\n",
    "            print(f\"Loading {model_name}...\")\n",
    "            \n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name, \n",
    "                torch_dtype=torch.float16,\n",
    "                trust_remote_code=True\n",
    "            ).to(device)\n",
    "            \n",
    "            tokenizer = AutoTokenizer.from_pretrained(\n",
    "                model_name,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            \n",
    "            self.experts.append(model)\n",
    "            self.tokenizers.append(tokenizer)\n",
    "            \n",
    "        # Create embedding layer for router (using embeddings from first model)\n",
    "        self.embedding = self.experts[0].get_input_embeddings()\n",
    "        \n",
    "        # Create router\n",
    "        self.router = SparseRouterMoE(router_dim, self.num_experts, k=k)\n",
    "        \n",
    "    def get_router_input(self, input_ids):\n",
    "        \"\"\"\n",
    "        Extract features for routing decision using embeddings\n",
    "        \"\"\"\n",
    "        # Get embeddings using the first model's embedding layer\n",
    "        emb = self.embedding(input_ids)\n",
    "        \n",
    "        # Use mean of sequence embeddings for routing decision\n",
    "        pooled = torch.mean(emb, dim=1)\n",
    "        \n",
    "        return pooled\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None, return_expert_weights=False):\n",
    "        \"\"\"\n",
    "        Forward pass through MoE ensemble\n",
    "        \"\"\"\n",
    "        batch_size = input_ids.shape[0]\n",
    "        \n",
    "        # Get router input representation\n",
    "        router_input = self.get_router_input(input_ids)\n",
    "        \n",
    "        # Get expert selection from router\n",
    "        dispatch_tensor, top_k_probs, top_k_indices, router_logits = self.router(router_input)\n",
    "        \n",
    "        # Prepare container for combined output\n",
    "        expert_outputs = []\n",
    "        \n",
    "        # Get outputs from selected experts\n",
    "        for batch_idx in range(batch_size):\n",
    "            # Get the experts selected for this input\n",
    "            selected_experts = top_k_indices[batch_idx]\n",
    "            expert_weights = top_k_probs[batch_idx]\n",
    "            \n",
    "            # Get outputs from selected experts\n",
    "            batch_input_ids = input_ids[batch_idx:batch_idx+1]\n",
    "            batch_attention_mask = None\n",
    "            if attention_mask is not None:\n",
    "                batch_attention_mask = attention_mask[batch_idx:batch_idx+1]\n",
    "            \n",
    "            # Calculate weighted sum of expert outputs\n",
    "            batch_outputs = []\n",
    "            for expert_idx, weight in zip(selected_experts, expert_weights):\n",
    "                expert = self.experts[expert_idx]\n",
    "                \n",
    "                # Get output from expert\n",
    "                with torch.no_grad():\n",
    "                    expert_output = expert(\n",
    "                        input_ids=batch_input_ids,\n",
    "                        attention_mask=batch_attention_mask\n",
    "                    )\n",
    "                    \n",
    "                # Scale output by expert weight\n",
    "                weighted_output = expert_output.logits * weight\n",
    "                batch_outputs.append(weighted_output)\n",
    "            \n",
    "            # Combine outputs\n",
    "            combined_output = torch.sum(torch.stack(batch_outputs), dim=0)\n",
    "            expert_outputs.append(combined_output)\n",
    "        \n",
    "        # Stack outputs from all batches\n",
    "        combined_logits = torch.cat(expert_outputs, dim=0)\n",
    "        \n",
    "        if return_expert_weights:\n",
    "            return combined_logits, dispatch_tensor\n",
    "        return combined_logits\n",
    "\n",
    "    def generate(self, input_text, max_length=512, temperature=0.7, top_p=0.9, \n",
    "                 num_return_sequences=1, return_expert_info=False):\n",
    "        \"\"\"\n",
    "        Generate text using the MoE ensemble\n",
    "        \"\"\"\n",
    "        # Tokenize input text (using first tokenizer as default)\n",
    "        tokenizer = self.tokenizers[0]\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\").to(self.device)\n",
    "        input_ids = inputs.input_ids\n",
    "        \n",
    "        # Get router input and determine which experts to use\n",
    "        router_input = self.get_router_input(input_ids)\n",
    "        dispatch_tensor, top_k_probs, top_k_indices, _ = self.router(router_input)\n",
    "        \n",
    "        # For simplicity, use the top expert for generation\n",
    "        expert_idx = top_k_indices[0][0].item()\n",
    "        weight = top_k_probs[0][0].item()\n",
    "        expert = self.experts[expert_idx]\n",
    "        expert_tokenizer = self.tokenizers[expert_idx]\n",
    "        \n",
    "        print(f\"Using expert {expert_idx} with weight {weight:.4f}\")\n",
    "        \n",
    "        # Generate text with the selected expert\n",
    "        with torch.no_grad():\n",
    "            outputs = expert.generate(\n",
    "                input_ids,\n",
    "                max_length=max_length,\n",
    "                do_sample=True,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                num_return_sequences=num_return_sequences,\n",
    "            )\n",
    "        \n",
    "        # Decode generated text\n",
    "        generated_texts = expert_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        \n",
    "        if return_expert_info:\n",
    "            expert_info = {\n",
    "                \"expert_idx\": expert_idx,\n",
    "                \"expert_weight\": weight,\n",
    "                \"expert_model\": expert.__class__.__name__,\n",
    "                \"dispatch_tensor\": dispatch_tensor.detach().cpu().numpy(),\n",
    "            }\n",
    "            return generated_texts, expert_info\n",
    "        \n",
    "        return generated_texts\n",
    "\n",
    "# Helper function to save and load the MoE ensemble\n",
    "def save_moe_ensemble(model, path):\n",
    "    \"\"\"Save MoE ensemble model\"\"\"\n",
    "    torch.save({\n",
    "        'router_state_dict': model.router.state_dict(),\n",
    "        'model_config': {\n",
    "            'num_experts': model.num_experts,\n",
    "            'k': model.k,\n",
    "        }\n",
    "    }, path)\n",
    "    \n",
    "def load_moe_ensemble(path, expert_models, device=\"cuda\", router_dim=768):\n",
    "    \"\"\"Load MoE ensemble model\"\"\"\n",
    "    checkpoint = torch.load(path)\n",
    "    \n",
    "    # Create new ensemble\n",
    "    model = QwenMoEEnsemble(\n",
    "        expert_models=expert_models,\n",
    "        device=device,\n",
    "        router_dim=router_dim,\n",
    "        k=checkpoint['model_config']['k']\n",
    "    )\n",
    "    \n",
    "    # Load router state\n",
    "    model.router.load_state_dict(checkpoint['router_state_dict'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Training function for fine-tuning the router\n",
    "def train_moe_router(model, train_dataset, optimizer, num_epochs=3, batch_size=8):\n",
    "    \"\"\"\n",
    "    Train the router of the MoE ensemble\n",
    "    \n",
    "    Args:\n",
    "        model: The MoE ensemble model\n",
    "        train_dataset: Dataset of (input_text, expert_label) pairs\n",
    "        optimizer: Optimizer for the router\n",
    "        num_epochs: Number of training epochs\n",
    "        batch_size: Batch size for training\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    # Place only the router parameters in training mode\n",
    "    for expert in model.experts:\n",
    "        expert.eval()\n",
    "    \n",
    "    # Create data loader\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            # Unpack batch\n",
    "            input_texts, expert_labels = batch\n",
    "            \n",
    "            # Tokenize input texts\n",
    "            tokenizer = model.tokenizers[0]\n",
    "            inputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "            input_ids = inputs.input_ids\n",
    "            \n",
    "            # Get router input\n",
    "            router_input = model.get_router_input(input_ids)\n",
    "            \n",
    "            # Get router logits\n",
    "            _, _, _, router_logits = model.router(router_input)\n",
    "            \n",
    "            # Compute cross-entropy loss for router\n",
    "            expert_labels = expert_labels.to(model.device)\n",
    "            loss = F.cross_entropy(router_logits, expert_labels)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define expert models\n",
    "    expert_models = [\n",
    "        {\"source_model\": \"Qwen/Qwen2.5-1.5B-Instruct\"},\n",
    "        {\"source_model\": \"Qwen/Qwen2.5-Coder-1.5B-Instruct\"},\n",
    "        {\"source_model\": \"Qwen/Qwen2.5-Math-1.5B-Instruct\"}\n",
    "    ]\n",
    "    \n",
    "    # Create MoE ensemble\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    moe_model = QwenMoEEnsemble(expert_models, device=device, k=1)\n",
    "    \n",
    "    # # Example input for testing\n",
    "    # input_text = \"Write a function to find the prime numbers up to n\"\n",
    "    \n",
    "    # # Generate text with the MoE ensemble\n",
    "    # generated_texts, expert_info = moe_model.generate(\n",
    "    #     input_text, max_length=512, return_expert_info=True\n",
    "    # )\n",
    "    \n",
    "    # print(f\"Input: {input_text}\")\n",
    "    # print(f\"Using expert: {expert_info['expert_idx']} (weight: {expert_info['expert_weight']:.4f})\")\n",
    "    # print(f\"Generated text: {generated_texts[0]}\")\n",
    "    \n",
    "    # # Example of how to create a training dataset\n",
    "    # class ExampleDataset(torch.utils.data.Dataset):\n",
    "    #     def __init__(self, examples):\n",
    "    #         self.examples = examples\n",
    "            \n",
    "    #     def __len__(self):\n",
    "    #         return len(self.examples)\n",
    "            \n",
    "    #     def __getitem__(self, idx):\n",
    "    #         return self.examples[idx]\n",
    "    \n",
    "    # # Example training data (input_text, expert_label)\n",
    "    # training_examples = [\n",
    "    #     (\"Write a function to calculate fibonacci numbers\", 1),  # Programming → Coder model\n",
    "    #     (\"Solve the equation 3x + 5 = 20\", 2),                   # Math → Math model\n",
    "    #     (\"Tell me about climate change\", 0),                     # General → Instruct model\n",
    "    # ]\n",
    "    \n",
    "    # # Create training dataset\n",
    "    # train_dataset = ExampleDataset(training_examples)\n",
    "    \n",
    "    # # Create optimizer for router parameters\n",
    "    # optimizer = torch.optim.Adam(moe_model.router.parameters(), lr=1e-4)\n",
    "    \n",
    "    # # Train router\n",
    "    # # Uncomment to train:\n",
    "    # # train_moe_router(moe_model, train_dataset, optimizer, num_epochs=5)\n",
    "    \n",
    "    # # Save the trained ensemble\n",
    "    # # save_moe_ensemble(moe_model, \"qwen_moe_ensemble.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
