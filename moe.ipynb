{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "class SparseRouterMoE(nn.Module):\n",
    "    \"\"\"\n",
    "    Sparse router for Mixture of Experts that decides which expert to use for each input.\n",
    "    Uses a k-sparse activation where only top-k experts are selected.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, num_experts, k=1):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.num_experts = num_experts\n",
    "        self.k = k  # Number of experts to route to\n",
    "        \n",
    "        # Router network (maps input to expert selection probabilities)\n",
    "        self.router = nn.Linear(input_size, num_experts)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Input embedding or hidden state\n",
    "        Returns:\n",
    "          - dispatch_tensor: Binary tensor indicating which experts to use\n",
    "          - combine_tensor: Weights for combining expert outputs\n",
    "          - router_logits: Raw logits from router\n",
    "        \"\"\"\n",
    "        # Get router logits and probabilities\n",
    "        router_logits = self.router(x)  # [batch_size, seq_len, num_experts]\n",
    "        \n",
    "        # Apply softmax to get expert selection probabilities\n",
    "        router_probs = F.softmax(router_logits, dim=-1)\n",
    "        \n",
    "        # Select top-k experts\n",
    "        top_k_probs, top_k_indices = torch.topk(router_probs, self.k, dim=-1)\n",
    "        \n",
    "        # Normalize the top-k probabilities\n",
    "        top_k_probs_sum = torch.sum(top_k_probs, dim=-1, keepdim=True)\n",
    "        top_k_probs = top_k_probs / top_k_probs_sum\n",
    "        \n",
    "        # Create dispatch tensor (indicates which experts to use)\n",
    "        dispatch_tensor = torch.zeros_like(router_probs)\n",
    "        \n",
    "        # Use scatter to place top-k probabilities in the dispatch tensor\n",
    "        dispatch_tensor.scatter_(-1, top_k_indices, top_k_probs)\n",
    "        \n",
    "        return dispatch_tensor, top_k_probs, top_k_indices, router_logits\n",
    "\n",
    "class MoEInput:\n",
    "    \"\"\"Helper class to store input representation for routing\"\"\"\n",
    "    def __init__(self, input_ids, attention_mask=None):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "\n",
    "class QwenMoEEnsemble(nn.Module):\n",
    "    \"\"\"\n",
    "    Ensemble model using Sparse Mixture of Experts architecture with \n",
    "    Qwen models as experts.\n",
    "    \"\"\"\n",
    "    def __init__(self, expert_models, device=\"cuda\", router_dim=768, k=1):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.num_experts = len(expert_models)\n",
    "        self.k = k\n",
    "        \n",
    "        # Load expert models and tokenizers\n",
    "        self.experts = []\n",
    "        self.tokenizers = []\n",
    "        \n",
    "        print(f\"Loading {self.num_experts} expert models...\")\n",
    "        for model_info in expert_models:\n",
    "            model_name = model_info[\"source_model\"]\n",
    "            print(f\"Loading {model_name}...\")\n",
    "            \n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name, \n",
    "                torch_dtype=torch.float16,\n",
    "                trust_remote_code=True\n",
    "            ).to(device)\n",
    "            \n",
    "            tokenizer = AutoTokenizer.from_pretrained(\n",
    "                model_name,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            \n",
    "            self.experts.append(model)\n",
    "            self.tokenizers.append(tokenizer)\n",
    "            \n",
    "        # Create embedding layer for router (using embeddings from first model)\n",
    "        self.embedding = self.experts[0].get_input_embeddings()\n",
    "        \n",
    "        # Create router\n",
    "        self.router = SparseRouterMoE(router_dim, self.num_experts, k=k)\n",
    "        \n",
    "    def get_router_input(self, input_ids):\n",
    "        \"\"\"\n",
    "        Extract features for routing decision using embeddings\n",
    "        \"\"\"\n",
    "        # Get embeddings using the first model's embedding layer\n",
    "        emb = self.embedding(input_ids)\n",
    "        \n",
    "        # Use mean of sequence embeddings for routing decision\n",
    "        pooled = torch.mean(emb, dim=1)\n",
    "        \n",
    "        return pooled\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None, return_expert_weights=False):\n",
    "        \"\"\"\n",
    "        Forward pass through MoE ensemble\n",
    "        \"\"\"\n",
    "        batch_size = input_ids.shape[0]\n",
    "        \n",
    "        # Get router input representation\n",
    "        router_input = self.get_router_input(input_ids)\n",
    "        \n",
    "        # Get expert selection from router\n",
    "        dispatch_tensor, top_k_probs, top_k_indices, router_logits = self.router(router_input)\n",
    "        \n",
    "        # Prepare container for combined output\n",
    "        expert_outputs = []\n",
    "        \n",
    "        # Get outputs from selected experts\n",
    "        for batch_idx in range(batch_size):\n",
    "            # Get the experts selected for this input\n",
    "            selected_experts = top_k_indices[batch_idx]\n",
    "            expert_weights = top_k_probs[batch_idx]\n",
    "            \n",
    "            # Get outputs from selected experts\n",
    "            batch_input_ids = input_ids[batch_idx:batch_idx+1]\n",
    "            batch_attention_mask = None\n",
    "            if attention_mask is not None:\n",
    "                batch_attention_mask = attention_mask[batch_idx:batch_idx+1]\n",
    "            \n",
    "            # Calculate weighted sum of expert outputs\n",
    "            batch_outputs = []\n",
    "            for expert_idx, weight in zip(selected_experts, expert_weights):\n",
    "                expert = self.experts[expert_idx]\n",
    "                \n",
    "                # Get output from expert\n",
    "                with torch.no_grad():\n",
    "                    expert_output = expert(\n",
    "                        input_ids=batch_input_ids,\n",
    "                        attention_mask=batch_attention_mask\n",
    "                    )\n",
    "                    \n",
    "                # Scale output by expert weight\n",
    "                weighted_output = expert_output.logits * weight\n",
    "                batch_outputs.append(weighted_output)\n",
    "            \n",
    "            # Combine outputs\n",
    "            combined_output = torch.sum(torch.stack(batch_outputs), dim=0)\n",
    "            expert_outputs.append(combined_output)\n",
    "        \n",
    "        # Stack outputs from all batches\n",
    "        combined_logits = torch.cat(expert_outputs, dim=0)\n",
    "        \n",
    "        if return_expert_weights:\n",
    "            return combined_logits, dispatch_tensor\n",
    "        return combined_logits\n",
    "\n",
    "    def generate(self, input_text, max_length=512, temperature=0.7, top_p=0.9, \n",
    "                 num_return_sequences=1, return_expert_info=False):\n",
    "        \"\"\"\n",
    "        Generate text using the MoE ensemble\n",
    "        \"\"\"\n",
    "        # Tokenize input text (using first tokenizer as default)\n",
    "        tokenizer = self.tokenizers[0]\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\").to(self.device)\n",
    "        input_ids = inputs.input_ids\n",
    "        \n",
    "        # Get router input and determine which experts to use\n",
    "        router_input = self.get_router_input(input_ids)\n",
    "        dispatch_tensor, top_k_probs, top_k_indices, _ = self.router(router_input)\n",
    "        \n",
    "        # For simplicity, use the top expert for generation\n",
    "        expert_idx = top_k_indices[0][0].item()\n",
    "        weight = top_k_probs[0][0].item()\n",
    "        expert = self.experts[expert_idx]\n",
    "        expert_tokenizer = self.tokenizers[expert_idx]\n",
    "        \n",
    "        print(f\"Using expert {expert_idx} with weight {weight:.4f}\")\n",
    "        \n",
    "        # Generate text with the selected expert\n",
    "        with torch.no_grad():\n",
    "            outputs = expert.generate(\n",
    "                input_ids,\n",
    "                max_length=max_length,\n",
    "                do_sample=True,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                num_return_sequences=num_return_sequences,\n",
    "            )\n",
    "        \n",
    "        # Decode generated text\n",
    "        generated_texts = expert_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        \n",
    "        if return_expert_info:\n",
    "            expert_info = {\n",
    "                \"expert_idx\": expert_idx,\n",
    "                \"expert_weight\": weight,\n",
    "                \"expert_model\": expert.__class__.__name__,\n",
    "                \"dispatch_tensor\": dispatch_tensor.detach().cpu().numpy(),\n",
    "            }\n",
    "            return generated_texts, expert_info\n",
    "        \n",
    "        return generated_texts\n",
    "\n",
    "# Helper function to save and load the MoE ensemble\n",
    "def save_moe_ensemble(model, path):\n",
    "    \"\"\"Save MoE ensemble model\"\"\"\n",
    "    torch.save({\n",
    "        'router_state_dict': model.router.state_dict(),\n",
    "        'model_config': {\n",
    "            'num_experts': model.num_experts,\n",
    "            'k': model.k,\n",
    "        }\n",
    "    }, path)\n",
    "    \n",
    "def load_moe_ensemble(path, expert_models, device=\"cuda\", router_dim=768):\n",
    "    \"\"\"Load MoE ensemble model\"\"\"\n",
    "    checkpoint = torch.load(path)\n",
    "    \n",
    "    # Create new ensemble\n",
    "    model = QwenMoEEnsemble(\n",
    "        expert_models=expert_models,\n",
    "        device=device,\n",
    "        router_dim=router_dim,\n",
    "        k=checkpoint['model_config']['k']\n",
    "    )\n",
    "    \n",
    "    # Load router state\n",
    "    model.router.load_state_dict(checkpoint['router_state_dict'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Training function for fine-tuning the router\n",
    "def train_moe_router(model, train_dataset, optimizer, num_epochs=3, batch_size=8):\n",
    "    \"\"\"\n",
    "    Train the router of the MoE ensemble\n",
    "    \n",
    "    Args:\n",
    "        model: The MoE ensemble model\n",
    "        train_dataset: Dataset of (input_text, expert_label) pairs\n",
    "        optimizer: Optimizer for the router\n",
    "        num_epochs: Number of training epochs\n",
    "        batch_size: Batch size for training\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    # Place only the router parameters in training mode\n",
    "    for expert in model.experts:\n",
    "        expert.eval()\n",
    "    \n",
    "    # Create data loader\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            # Unpack batch\n",
    "            input_texts, expert_labels = batch\n",
    "            \n",
    "            # Tokenize input texts\n",
    "            tokenizer = model.tokenizers[0]\n",
    "            inputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "            input_ids = inputs.input_ids\n",
    "            \n",
    "            # Get router input\n",
    "            router_input = model.get_router_input(input_ids)\n",
    "            \n",
    "            # Get router logits\n",
    "            _, _, _, router_logits = model.router(router_input)\n",
    "            \n",
    "            # Compute cross-entropy loss for router\n",
    "            expert_labels = expert_labels.to(model.device)\n",
    "            loss = F.cross_entropy(router_logits, expert_labels)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define expert models\n",
    "    expert_models = [\n",
    "        {\"source_model\": \"Qwen/Qwen2.5-1.5B-Instruct\"},\n",
    "        {\"source_model\": \"Qwen/Qwen2.5-Coder-1.5B-Instruct\"},\n",
    "        {\"source_model\": \"Qwen/Qwen2.5-Math-1.5B-Instruct\"}\n",
    "    ]\n",
    "    \n",
    "    # Create MoE ensemble\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    moe_model = QwenMoEEnsemble(expert_models, device=device, k=1)\n",
    "    \n",
    "    # # Example input for testing\n",
    "    # input_text = \"Write a function to find the prime numbers up to n\"\n",
    "    \n",
    "    # # Generate text with the MoE ensemble\n",
    "    # generated_texts, expert_info = moe_model.generate(\n",
    "    #     input_text, max_length=512, return_expert_info=True\n",
    "    # )\n",
    "    \n",
    "    # print(f\"Input: {input_text}\")\n",
    "    # print(f\"Using expert: {expert_info['expert_idx']} (weight: {expert_info['expert_weight']:.4f})\")\n",
    "    # print(f\"Generated text: {generated_texts[0]}\")\n",
    "    \n",
    "    # # Example of how to create a training dataset\n",
    "    # class ExampleDataset(torch.utils.data.Dataset):\n",
    "    #     def __init__(self, examples):\n",
    "    #         self.examples = examples\n",
    "            \n",
    "    #     def __len__(self):\n",
    "    #         return len(self.examples)\n",
    "            \n",
    "    #     def __getitem__(self, idx):\n",
    "    #         return self.examples[idx]\n",
    "    \n",
    "    # # Example training data (input_text, expert_label)\n",
    "    # training_examples = [\n",
    "    #     (\"Write a function to calculate fibonacci numbers\", 1),  # Programming → Coder model\n",
    "    #     (\"Solve the equation 3x + 5 = 20\", 2),                   # Math → Math model\n",
    "    #     (\"Tell me about climate change\", 0),                     # General → Instruct model\n",
    "    # ]\n",
    "    \n",
    "    # # Create training dataset\n",
    "    # train_dataset = ExampleDataset(training_examples)\n",
    "    \n",
    "    # # Create optimizer for router parameters\n",
    "    # optimizer = torch.optim.Adam(moe_model.router.parameters(), lr=1e-4)\n",
    "    \n",
    "    # # Train router\n",
    "    # # Uncomment to train:\n",
    "    # # train_moe_router(moe_model, train_dataset, optimizer, num_epochs=5)\n",
    "    \n",
    "    # # Save the trained ensemble\n",
    "    # # save_moe_ensemble(moe_model, \"qwen_moe_ensemble.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "code_model_name = \"Qwen/Qwen2.5-Coder-1.5B-Instruct\"\n",
    "math_model_name = \"Qwen/Qwen2.5-Math-1.5B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f072dc5b4e344c3aac94f81b5c607c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b0d68cc47824738af7066eb1658be3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "facde9f6fa864c61b0c53ba957f34f6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d423dcf1d442437db67ce3fb84e979b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "112afa32cbf847d8bb0e37498d916eaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4288badaacc45d8b174d107580d17bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c97c5c8a6ca14aa6b3b9b765f718c28b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "292fe2ccdce24f32bc432cca62952c8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat_tokenizer = AutoTokenizer.from_pretrained(chat_model_name, trust_remote_code=True)\n",
    "code_tokenizer = AutoTokenizer.from_pretrained(code_model_name, trust_remote_code=True)\n",
    "math_tokenizer = AutoTokenizer.from_pretrained(math_model_name, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b128006e72e41bc839a101cb1e21a91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  13%|#2        | 388M/3.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0608e887cc9c40168a95734eb00857c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e4b4908356d4895aa15e11177ea9520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/656 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de78944d380d47ca9005fa756fd391ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "937afadc7e3845458f5333825ea4c32b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat_model = AutoModelForCausalLM.from_pretrained(chat_model_name)\n",
    "code_model = AutoModelForCausalLM.from_pretrained(code_model_name)\n",
    "math_model = AutoModelForCausalLM.from_pretrained(math_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{%- if tools %}\\n    {{- \\'<|im_start|>system\\\\n\\' }}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- messages[0][\\'content\\'] }}\\n    {%- else %}\\n        {{- \\'Please reason step by step, and put your final answer within \\\\\\\\boxed{}.\\' }}\\n    {%- endif %}\\n    {{- \"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\\\n\\' + messages[0][\\'content\\'] + \\'<|im_end|>\\\\n\\' }}\\n    {%- else %}\\n        {{- \\'<|im_start|>system\\\\nPlease reason step by step, and put your final answer within \\\\\\\\boxed{}.<|im_end|>\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + message.content + \\'<|im_end|>\\' + \\'\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {{- \\'<|im_start|>\\' + message.role }}\\n        {%- if message.content %}\\n            {{- \\'\\\\n\\' + message.content }}\\n        {%- endif %}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- \\'\\\\n<tool_call>\\\\n{\"name\": \"\\' }}\\n            {{- tool_call.name }}\\n            {{- \\'\", \"arguments\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \\'}\\\\n</tool_call>\\' }}\\n        {%- endfor %}\\n        {{- \\'<|im_end|>\\\\n\\' }}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\\n            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\\\n<tool_response>\\\\n\\' }}\\n        {{- message.content }}\\n        {{- \\'\\\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\\\n\\' }}\\n{%- endif %}\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math_tokenizer.chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "\n",
    "class ExpertRouter(nn.Module):\n",
    "    \"\"\"\n",
    "    Router network that determines which expert to use for a given input\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size: int, num_experts: int, top_k: int = 2):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        self.router = nn.Linear(hidden_size, num_experts)\n",
    "        \n",
    "    def forward(self, hidden_states: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Calculate routing scores\n",
    "        routing_logits = self.router(hidden_states)  # [batch_size, seq_len, num_experts]\n",
    "        \n",
    "        # Get top-k experts\n",
    "        routing_weights, expert_indices = torch.topk(\n",
    "            F.softmax(routing_logits, dim=-1), \n",
    "            self.top_k, \n",
    "            dim=-1\n",
    "        )\n",
    "        \n",
    "        # Normalize the routing weights\n",
    "        routing_weights = routing_weights / routing_weights.sum(dim=-1, keepdim=True)\n",
    "        \n",
    "        return routing_weights, expert_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Dict, Tuple, Optional, Union\n",
    "from dataclasses import dataclass\n",
    "from transformers import PreTrainedModel, Qwen2ForCausalLM, Qwen2Config, Qwen2Model\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from transformers.utils import logging\n",
    "\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "class MoERouter(nn.Module):\n",
    "    \"\"\"Router module that selects which experts to use for each token.\"\"\"\n",
    "    def __init__(self, hidden_size: int, num_experts: int, top_k: int = 2):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = min(top_k, num_experts)  # Ensure top_k doesn't exceed num_experts\n",
    "        self.router = nn.Linear(hidden_size, num_experts, bias=False)\n",
    "        \n",
    "    def forward(self, hidden_states: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states: [batch_size, seq_len, hidden_size]\n",
    "        Returns:\n",
    "            routing_weights: [batch_size, seq_len, top_k]\n",
    "            expert_indices: [batch_size, seq_len, top_k]\n",
    "        \"\"\"\n",
    "        # Get routing logits\n",
    "        routing_logits = self.router(hidden_states)  # [batch_size, seq_len, num_experts]\n",
    "        \n",
    "        # Apply softmax to get routing probabilities\n",
    "        routing_probs = F.softmax(routing_logits, dim=-1)\n",
    "        \n",
    "        # Get top-k experts and their weights\n",
    "        routing_weights, expert_indices = torch.topk(routing_probs, self.top_k, dim=-1)\n",
    "        \n",
    "        # Normalize weights to sum to 1\n",
    "        routing_weights = routing_weights / routing_weights.sum(dim=-1, keepdim=True)\n",
    "        \n",
    "        return routing_weights, expert_indices\n",
    "\n",
    "class Qwen2MoELayer(nn.Module):\n",
    "    \"\"\"MoE layer that replaces a standard transformer layer.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        expert_models: List[Qwen2ForCausalLM],\n",
    "        layer_idx: int,\n",
    "        hidden_size: int = 1536,\n",
    "        num_experts: int = 3,\n",
    "        top_k: int = 2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        self.layer_idx = layer_idx\n",
    "        \n",
    "        # Extract specific layer from each expert model\n",
    "        self.expert_layers = nn.ModuleList([\n",
    "            expert.model.layers[layer_idx] for expert in expert_models\n",
    "        ])\n",
    "        \n",
    "        # Router for selecting experts\n",
    "        self.router = MoERouter(hidden_size, num_experts, top_k)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "        use_cache: Optional[bool] = False,\n",
    "    ) -> Tuple:\n",
    "        # Route to experts\n",
    "        routing_weights, expert_indices = self.router(hidden_states)\n",
    "        \n",
    "        # Prepare output tensor\n",
    "        batch_size, seq_len, hidden_dim = hidden_states.shape\n",
    "        output = torch.zeros_like(hidden_states)\n",
    "        \n",
    "        # Process with each selected expert\n",
    "        for k in range(self.top_k):\n",
    "            for expert_idx in range(self.num_experts):\n",
    "                # Create mask for tokens routed to this expert\n",
    "                expert_mask = (expert_indices[..., k] == expert_idx).float().unsqueeze(-1)\n",
    "                \n",
    "                # Skip if no tokens are routed to this expert\n",
    "                if expert_mask.sum() == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Get weights for this expert\n",
    "                weight = routing_weights[..., k].unsqueeze(-1) * expert_mask\n",
    "                \n",
    "                # Forward through expert layer\n",
    "                expert_output = self.expert_layers[expert_idx](\n",
    "                    hidden_states,\n",
    "                    attention_mask=attention_mask,\n",
    "                    position_ids=position_ids,\n",
    "                    past_key_value=past_key_value,\n",
    "                    output_attentions=output_attentions,\n",
    "                    use_cache=use_cache,\n",
    "                )[0]  # Get only the hidden states\n",
    "                \n",
    "                # Weight and accumulate output\n",
    "                output += expert_output * weight\n",
    "        \n",
    "        # If using cache, need to return past_key_value\n",
    "        if use_cache:\n",
    "            return (output, past_key_value)\n",
    "        return (output,)\n",
    "\n",
    "class Qwen2MoEModel(nn.Module):\n",
    "    \"\"\"MoE version of Qwen2Model with specialized expert layers.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        expert_models: List[Qwen2ForCausalLM],\n",
    "        config: Qwen2Config,\n",
    "        moe_layers: List[int] = None,  # Layer indices to replace with MoE\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.num_experts = len(expert_models)\n",
    "        \n",
    "        # If no MoE layers specified, use every other layer starting with layer 1\n",
    "        if moe_layers is None:\n",
    "            moe_layers = list(range(1, config.num_hidden_layers, 2))\n",
    "        self.moe_layers = set(moe_layers)\n",
    "        \n",
    "        # Use the embedding layer from the first expert\n",
    "        self.embed_tokens = expert_models[0].model.embed_tokens\n",
    "        \n",
    "        # Create a mix of standard and MoE layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(config.num_hidden_layers):\n",
    "            if i in self.moe_layers:\n",
    "                # MoE layer\n",
    "                self.layers.append(\n",
    "                    Qwen2MoELayer(\n",
    "                        expert_models=expert_models,\n",
    "                        layer_idx=i,\n",
    "                        hidden_size=config.hidden_size,\n",
    "                        num_experts=self.num_experts,\n",
    "                        top_k=2,\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                # Standard layer (from first expert)\n",
    "                self.layers.append(expert_models[0].model.layers[i])\n",
    "        \n",
    "        # Normalization layer from first expert\n",
    "        self.norm = expert_models[0].model.norm\n",
    "        \n",
    "        # Copy rotary embeddings from first expert\n",
    "        self.rotary_emb = expert_models[0].model.rotary_emb\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ):\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds\")\n",
    "        \n",
    "        # Create embeddings if not provided\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.embed_tokens(input_ids)\n",
    "        \n",
    "        hidden_states = inputs_embeds\n",
    "        \n",
    "        # Initialize variables for outputs\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attns = () if output_attentions else None\n",
    "        next_decoder_cache = () if use_cache else None\n",
    "        \n",
    "        # Forward through all layers\n",
    "        for idx, decoder_layer in enumerate(self.layers):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states += (hidden_states,)\n",
    "            \n",
    "            past_key_value = past_key_values[idx] if past_key_values is not None else None\n",
    "            \n",
    "            # Layer forward pass\n",
    "            layer_outputs = decoder_layer(\n",
    "                hidden_states,\n",
    "                attention_mask=attention_mask,\n",
    "                position_ids=position_ids,\n",
    "                past_key_value=past_key_value,\n",
    "                output_attentions=output_attentions,\n",
    "                use_cache=use_cache,\n",
    "            )\n",
    "            \n",
    "            hidden_states = layer_outputs[0]\n",
    "            \n",
    "            if use_cache:\n",
    "                next_decoder_cache += (layer_outputs[1],)\n",
    "            \n",
    "            if output_attentions and idx not in self.moe_layers:\n",
    "                all_self_attns += (layer_outputs[1 if use_cache else 1],)\n",
    "        \n",
    "        # Final layer norm\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "        \n",
    "        # Add last hidden state\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states += (hidden_states,)\n",
    "        \n",
    "        next_cache = next_decoder_cache if use_cache else None\n",
    "        \n",
    "        if return_dict:\n",
    "            return {\n",
    "                \"last_hidden_state\": hidden_states,\n",
    "                \"past_key_values\": next_cache,\n",
    "                \"hidden_states\": all_hidden_states,\n",
    "                \"attentions\": all_self_attns,\n",
    "            }\n",
    "        return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n",
    "\n",
    "class Qwen2MoEForCausalLM(PreTrainedModel):\n",
    "    \"\"\"\n",
    "    Qwen2 model with Mixture of Experts for causal language modeling.\n",
    "    \"\"\"\n",
    "    config_class = Qwen2Config\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        chat_model_path: str,\n",
    "        code_model_path: str,\n",
    "        math_model_path: str,\n",
    "        config: Optional[Qwen2Config] = None,\n",
    "        moe_layers: List[int] = None,\n",
    "        device_map=\"auto\",\n",
    "    ):\n",
    "        # Load expert models with appropriate device mapping for large models\n",
    "        expert_models = []\n",
    "        for i, path in enumerate([chat_model_path, code_model_path, math_model_path]):\n",
    "            # Use different device mapping strategy for each model to prevent OOM\n",
    "            if device_map == \"auto\":\n",
    "                # Load each expert on a different CUDA device if multi-GPU setup or use disk offloading\n",
    "                expert_models.append(Qwen2ForCausalLM.from_pretrained(\n",
    "                    path,\n",
    "                    device_map=\"auto\",  # Automatic device mapping\n",
    "                    torch_dtype=torch.float16,  # Use half precision to save memory\n",
    "                    low_cpu_mem_usage=True,  # Lower CPU memory usage during loading\n",
    "                ))\n",
    "            else:\n",
    "                # Use the specified device map\n",
    "                expert_models.append(Qwen2ForCausalLM.from_pretrained(\n",
    "                    path,\n",
    "                    device_map=device_map,\n",
    "                    torch_dtype=torch.float16,\n",
    "                    low_cpu_mem_usage=True,\n",
    "                ))\n",
    "        \n",
    "        # Use config from first model if not provided\n",
    "        if config is None:\n",
    "            config = expert_models[0].config\n",
    "            \n",
    "        # Store the model paths in the config\n",
    "        config.expert_model_paths = {\n",
    "            'chat': chat_model_path,\n",
    "            'code': code_model_path,\n",
    "            'math': math_model_path\n",
    "        }\n",
    "        \n",
    "        # Store MoE layers in config\n",
    "        if moe_layers is not None:\n",
    "            config.moe_layers = moe_layers\n",
    "        \n",
    "        super().__init__(config)\n",
    "        \n",
    "        # Create the MoE model\n",
    "        self.model = Qwen2MoEModel(\n",
    "            expert_models=expert_models,\n",
    "            config=config,\n",
    "            moe_layers=moe_layers,\n",
    "        )\n",
    "        \n",
    "        # Use LM head from first expert\n",
    "        self.lm_head = expert_models[0].lm_head\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.post_init()\n",
    "    \n",
    "    def get_input_embeddings(self):\n",
    "        return self.model.embed_tokens\n",
    "    \n",
    "    def set_input_embeddings(self, value):\n",
    "        self.model.embed_tokens = value\n",
    "    \n",
    "    def get_output_embeddings(self):\n",
    "        return self.lm_head\n",
    "    \n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.lm_head = new_embeddings\n",
    "    \n",
    "    def set_decoder(self, decoder):\n",
    "        self.model = decoder\n",
    "    \n",
    "    def get_decoder(self):\n",
    "        return self.model\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        # Forward through model\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        \n",
    "        hidden_states = outputs[\"last_hidden_state\"]\n",
    "        \n",
    "        # Get logits\n",
    "        logits = self.lm_head(hidden_states)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            \n",
    "            # Flatten the tokens\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "        \n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[1:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "        \n",
    "        return CausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            past_key_values=outputs.get(\"past_key_values\"),\n",
    "            hidden_states=outputs.get(\"hidden_states\"),\n",
    "            attentions=outputs.get(\"attentions\"),\n",
    "        )\n",
    "    \n",
    "    def prepare_inputs_for_generation(\n",
    "        self, input_ids, past_key_values=None, attention_mask=None, inputs_embeds=None, **kwargs\n",
    "    ):\n",
    "        if past_key_values is not None:\n",
    "            input_ids = input_ids[:, -1:]\n",
    "        \n",
    "        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n",
    "        if inputs_embeds is not None and past_key_values is None:\n",
    "            model_inputs = {\"inputs_embeds\": inputs_embeds}\n",
    "        else:\n",
    "            model_inputs = {\"input_ids\": input_ids}\n",
    "        \n",
    "        model_inputs.update(\n",
    "            {\n",
    "                \"past_key_values\": past_key_values,\n",
    "                \"use_cache\": kwargs.get(\"use_cache\"),\n",
    "                \"attention_mask\": attention_mask,\n",
    "            }\n",
    "        )\n",
    "        return model_inputs\n",
    "    \n",
    "    @staticmethod\n",
    "    def _reorder_cache(past_key_values, beam_idx):\n",
    "        reordered_past = ()\n",
    "        for layer_past in past_key_values:\n",
    "            reordered_past += (\n",
    "                tuple(past_state.index_select(0, beam_idx) for past_state in layer_past),\n",
    "            )\n",
    "        return reordered_past\n",
    "        \n",
    "    def save_pretrained(self, save_directory, safe_serialization=False, max_shard_size=\"10GB\", **kwargs):\n",
    "        \"\"\"\n",
    "        Save the model using HF's PreTrainedModel.save_pretrained with safe_serialization=False\n",
    "        to avoid the shared weights error. Also supports model sharding for large models.\n",
    "        \n",
    "        Args:\n",
    "            save_directory: Directory to save the model to\n",
    "            safe_serialization: Set to False to avoid shared weights error\n",
    "            max_shard_size: Maximum size for each model shard (e.g., \"10GB\", \"5GB\")\n",
    "            **kwargs: Additional arguments to pass to save_pretrained\n",
    "        \"\"\"\n",
    "        # Explicitly set safe_serialization to False to handle shared weights\n",
    "        # Add max_shard_size to handle large models\n",
    "        super().save_pretrained(\n",
    "            save_directory, \n",
    "            safe_serialization=False,\n",
    "            max_shard_size=max_shard_size,\n",
    "            **kwargs\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Qwen2MoEForCausalLM model...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "OUTPUT_PATH = \"./qwen2_moe_model\"\n",
    "\n",
    "# Create the MoE model\n",
    "# By default, every other layer will be replaced with MoE layers\n",
    "print(\"Creating Qwen2MoEForCausalLM model...\")\n",
    "model = Qwen2MoEForCausalLM(\n",
    "    chat_model_path=chat_model_name,\n",
    "    code_model_path=chat_model_name,\n",
    "    math_model_path=math_model_name,\n",
    "    # Optional: Specify which layers to replace with MoE\n",
    "    # moe_layers=[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./qwen2_5_moe_model\\\\tokenizer_config.json',\n",
       " './qwen2_5_moe_model\\\\special_tokens_map.json',\n",
       " './qwen2_5_moe_model\\\\vocab.json',\n",
       " './qwen2_5_moe_model\\\\merges.txt',\n",
       " './qwen2_5_moe_model\\\\added_tokens.json',\n",
       " './qwen2_5_moe_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUTPUT_PATH = \"./qwen2_5_moe_model\"\n",
    "model.save_pretrained(OUTPUT_PATH)\n",
    "tokenizer = AutoTokenizer.from_pretrained(chat_model_name, trust_remote_code=True)\n",
    "tokenizer.save_pretrained(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
