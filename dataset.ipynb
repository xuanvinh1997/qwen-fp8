{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in ./.local/lib/python3.10/site-packages (3.5.0)\n",
      "Requirement already satisfied: pandas in /usr/lib/python3/dist-packages (1.3.5)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp310-cp310-manylinux2014_aarch64.manylinux_2_17_aarch64.whl.metadata (89 kB)\n",
      "Requirement already satisfied: numpy<2 in ./.local/lib/python3.10/site-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib in /usr/lib/python3/dist-packages (3.5.1)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.1-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: seaborn in ./.local/lib/python3.10/site-packages (0.13.2)\n",
      "Requirement already satisfied: sentence-transformers in ./.local/lib/python3.10/site-packages (4.0.2)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.local/lib/python3.10/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.local/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./.local/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./.local/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in ./.local/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.local/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/lib/python3/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in ./.local/lib/python3.10/site-packages (from datasets) (3.11.16)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in ./.local/lib/python3.10/site-packages (from datasets) (0.30.1)\n",
      "Requirement already satisfied: packaging in /usr/lib/python3/dist-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from datasets) (5.4.1)\n",
      "Collecting python-dateutil>=2.8.2 (from pandas)\n",
      "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas) (2022.1)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.1-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/lib/python3/dist-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/lib/python3/dist-packages (from matplotlib) (4.29.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: pillow>=8 in /usr/lib/python3/dist-packages (from matplotlib) (9.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in ./.local/lib/python3.10/site-packages (from sentence-transformers) (4.51.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/lib/python3/dist-packages (from sentence-transformers) (2.6.0)\n",
      "Requirement already satisfied: scikit-learn in ./.local/lib/python3.10/site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /usr/lib/python3/dist-packages (from sentence-transformers) (1.8.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in ./.local/lib/python3.10/site-packages (from sentence-transformers) (4.13.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp->datasets) (21.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets) (6.3.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.19.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets) (2020.6.20)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.local/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/lib/python3/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
      "Downloading pandas-2.2.3-cp310-cp310-manylinux2014_aarch64.manylinux_2_17_aarch64.whl (66.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 MB\u001b[0m \u001b[31m367.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading matplotlib-3.10.1-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (8.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m225.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.1-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (312 kB)\n",
      "Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[33mWARNING: Error parsing dependencies of flatbuffers: Invalid version: '1.12.1-git20200711.33e2d80-dfsg1-0.6'\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: tzdata, python-dateutil, contourpy, pandas, matplotlib\n",
      "Successfully installed contourpy-1.3.1 matplotlib-3.10.1 pandas-2.2.3 python-dateutil-2.9.0.post0 tzdata-2025.2\n"
     ]
    }
   ],
   "source": [
    "!pip install -U datasets pandas \"numpy<2\" matplotlib seaborn sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-06 16:14:22,849] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n",
      "Using the latest cached version of the dataset since vinhpx/math_natural_reasoning couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /home/ubuntu/.cache/huggingface/datasets/vinhpx___math_natural_reasoning/default/0.0.0/80ca8b426eae79f62551418bf77ed94abb6184bd (last modified on Sun Apr  6 16:13:48 2025).\n",
      "Map:  12%|█▏        | 300000/2559963 [04:51<35:14, 1068.87 examples/s]"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "import faiss\n",
    "from functools import partial\n",
    "\n",
    "# Function to generate embeddings in parallel\n",
    "def embed_text(examples, model_name='all-MiniLM-L6-v2'):\n",
    "    model = SentenceTransformer(model_name, device='cuda', trust_remote_code=True)\n",
    "\n",
    "    # Assuming 'question' is the column containing sentences\n",
    "    embeddings = model.encode(examples['question'])\n",
    "    return {'embeddings': embeddings}\n",
    "\n",
    "# Function to find optimal K\n",
    "def find_optimal_k(embeddings, max_k=20, min_k=2):\n",
    "    # Combine multiple metrics for robust evaluation\n",
    "    results = {}\n",
    "    \n",
    "    for k in range(min_k, max_k+1):\n",
    "        # Use FAISS for faster clustering\n",
    "        kmeans = faiss.Kmeans(d=embeddings.shape[1], k=k, niter=20)\n",
    "        kmeans.train(embeddings)\n",
    "        _, labels = kmeans.index.search(embeddings, 1)\n",
    "        labels = labels.reshape(-1)\n",
    "        \n",
    "        # Calculate cluster quality metrics\n",
    "        sil_score = silhouette_score(embeddings, labels)\n",
    "        ch_score = calinski_harabasz_score(embeddings, labels)\n",
    "        db_score = davies_bouldin_score(embeddings, labels)\n",
    "        \n",
    "        # Normalized scores (higher is better for all)\n",
    "        results[k] = sil_score + ch_score/1000 - db_score\n",
    "    \n",
    "    return max(results, key=results.get)\n",
    "\n",
    "# Main processing pipeline\n",
    "def process_dataset(dataset_name, text_column='question', model_name='all-MiniLM-L6-v2', \n",
    "                   dimension_reduction=50, num_proc=40, batch_size=1000):\n",
    "    \n",
    "    # 1. Load the dataset with multi-processing\n",
    "    dataset = load_dataset(dataset_name)\n",
    "    if isinstance(dataset, dict):\n",
    "        # If dataset has splits, use the largest one\n",
    "        largest_split = max(dataset.keys(), key=lambda k: len(dataset[k]))\n",
    "        dataset = dataset[largest_split]\n",
    "    \n",
    "    # 2. Generate embeddings in parallel using all processors\n",
    "    embed_function = partial(embed_text, model_name=model_name)\n",
    "    dataset = dataset.map(\n",
    "        embed_function,\n",
    "        batched=True,\n",
    "        batch_size=batch_size,\n",
    "        num_proc=num_proc\n",
    "    )\n",
    "    \n",
    "    # 3. Convert to numpy array for further processing\n",
    "    embeddings = np.array(dataset['embeddings'])\n",
    "    \n",
    "    # 4. Dimension reduction with UMAP if needed (for very large embeddings)\n",
    "    if dimension_reduction and dimension_reduction < embeddings.shape[1]:\n",
    "        reducer = umap.UMAP(n_components=dimension_reduction, n_neighbors=15, \n",
    "                           min_dist=0.1, random_state=42, n_jobs=num_proc)\n",
    "        embeddings = reducer.fit_transform(embeddings)\n",
    "    \n",
    "    # 5. Convert to float32 for FAISS compatibility\n",
    "    embeddings = embeddings.astype('float32')\n",
    "    \n",
    "    # 6. Find optimal K\n",
    "    optimal_k = find_optimal_k(embeddings)\n",
    "    print(f\"Optimal number of clusters: {optimal_k}\")\n",
    "    \n",
    "    # 7. Final clustering with optimal K\n",
    "    kmeans = faiss.Kmeans(d=embeddings.shape[1], k=optimal_k, niter=20)\n",
    "    kmeans.train(embeddings)\n",
    "    _, labels = kmeans.index.search(embeddings, 1)\n",
    "    labels = labels.reshape(-1)\n",
    "    \n",
    "    # 8. Add cluster labels back to dataset\n",
    "    dataset = dataset.add_column(\"cluster\", labels.tolist())\n",
    "    \n",
    "    return dataset, labels, kmeans.centroids\n",
    "\n",
    "# Visualization function\n",
    "def visualize_clusters(embeddings, labels, method='umap'):\n",
    "    # Create a 2D visualization for exploration\n",
    "    reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "    reduced = reducer.fit_transform(embeddings)\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.scatter(reduced[:, 0], reduced[:, 1], c=labels, cmap='tab20', s=10, alpha=0.7)\n",
    "    plt.colorbar()\n",
    "    plt.title(f'Cluster Visualization ({len(np.unique(labels))} clusters)')\n",
    "    plt.show()\n",
    "    \n",
    "    return reduced\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace with your actual dataset name\n",
    "    dataset_name = \"vinhpx/math_natural_reasoning\"  \n",
    "    \n",
    "    # Process with optimal parameters\n",
    "    clustered_dataset, labels, centroids = process_dataset(\n",
    "        dataset_name=dataset_name,\n",
    "        text_column='question',  # Replace with your text column name\n",
    "        model_name='all-mpnet-base-v2',  # Sentence transformer model\n",
    "        dimension_reduction=None,  # Reduce dimensions for faster processing\n",
    "        num_proc=1,  # Use all 40 processors\n",
    "        batch_size=100000  # Adjust batch size based on memory availability\n",
    "    )\n",
    "    \n",
    "    # Get original embeddings for visualization\n",
    "    embeddings = np.array(clustered_dataset['embeddings'])\n",
    "    \n",
    "    # Visualize results\n",
    "    visualize_clusters(embeddings, labels)\n",
    "    \n",
    "    # Print some stats about clusters\n",
    "    for cluster_id in range(len(np.unique(labels))):\n",
    "        count = np.sum(labels == cluster_id)\n",
    "        percentage = count / len(labels) * 100\n",
    "        print(f\"Cluster {cluster_id}: {count} items ({percentage:.2f}%)\")\n",
    "        \n",
    "    # Get examples from each cluster\n",
    "    for cluster_id in range(min(3, len(np.unique(labels)))):  # Show first 3 clusters\n",
    "        examples = clustered_dataset.filter(lambda x: x['cluster'] == cluster_id).select(range(5))\n",
    "        print(f\"\\nCluster {cluster_id} examples:\")\n",
    "        for ex in examples:\n",
    "            print(f\"- {ex['question'][:100]}...\")  # Show first 100 chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
