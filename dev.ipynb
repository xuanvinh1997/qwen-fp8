{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Qwen2MoeForCausalLM\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "math_model_name = \"Qwen/Qwen2.5-Math-1.5B-Instruct\"\n",
    "coder_model_name = \"Qwen/Qwen2.5-Coder-1.5B-Instruct\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(chat_model_name)\n",
    "# chat_model = AutoModelForCausalLM.from_pretrained(chat_model_name, device_map=\"auto\")\n",
    "# math_model = AutoModelForCausalLM.from_pretrained(math_model_name, device_map=\"auto\")\n",
    "# coder_model = AutoModelForCausalLM.from_pretrained(coder_model_name, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 1536)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedModel, PreTrainedTokenizerBase\n",
    "from transformers import Qwen2MoeForCausalLM, Qwen2MoeConfig, Qwen2Model, Qwen2Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.qwen2.modeling_qwen2 import (\n",
    "    Qwen2Attention,\n",
    "    Qwen2RMSNorm,\n",
    "    Qwen2RotaryEmbedding,\n",
    "    Qwen2MLP,\n",
    ")\n",
    "from transformers import Qwen2MoeConfig\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Qwen2_5MoEConfig(Qwen2MoeConfig):\n",
    "    model_type = \"qwen2_5moe\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_experts=8,\n",
    "        top_k=2,\n",
    "        capacity_factor=1.5,\n",
    "        aux_loss_weight=0.01,\n",
    "        router_jitter=0.01,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        self.capacity_factor = capacity_factor\n",
    "        self.aux_loss_weight = aux_loss_weight\n",
    "        self.router_jitter = router_jitter\n",
    "\n",
    "        self.dtype = kwargs.get(\"dtype\", torch.float32)\n",
    "        self.dropout_rate = kwargs.get(\"dropout_rate\", 0.01)\n",
    "\n",
    "\n",
    "class Qwen2_5MoEExpertRouter(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Mixture of Experts Router Layer\n",
    "\n",
    "    Takes attention outputs as input and routes to top-k MLPs (experts)\n",
    "    using a learned routing mechanism.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,  # Dimension of input (from attention layer)\n",
    "        num_experts,  # Total number of experts available\n",
    "        top_k=1,  # Number of experts to route each token to\n",
    "        capacity_factor=1.5,  # Scaling factor for expert capacity\n",
    "        aux_loss_weight=0.01,  # Weight for auxiliary load balancing loss\n",
    "        router_jitter=0.01,  # Optional noise to add during training\n",
    "        dtype=torch.float32,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        self.capacity_factor = capacity_factor\n",
    "        self.aux_loss_weight = aux_loss_weight\n",
    "        self.router_jitter = router_jitter\n",
    "        self.dtype = dtype\n",
    "\n",
    "        # Router projection layer: maps input to expert selection logits\n",
    "        self.router = nn.Linear(input_dim, num_experts, bias=False, dtype=dtype)\n",
    "\n",
    "        # Initialize with small weights to encourage equal expert usage early in training\n",
    "        nn.init.normal_(self.router.weight, mean=0.0, std=0.01)\n",
    "\n",
    "    def forward(self, inp, training=True):\n",
    "        \"\"\"\n",
    "        Forward pass for the router\n",
    "\n",
    "        Args:\n",
    "            inp: Tensor of shape [batch_size, seq_len, input_dim] from attention layer\n",
    "            training: Whether the model is in training mode\n",
    "\n",
    "        Returns:\n",
    "            dispatch_tensor: Sparse tensor for dispatching inputs to experts\n",
    "            combine_tensor: Sparse tensor for combining expert outputs\n",
    "            router_logits: Raw router logits\n",
    "            aux_loss: Load balancing auxiliary loss\n",
    "        \"\"\"\n",
    "        # Get shape information\n",
    "        batch_size, seq_len, _ = inp.shape\n",
    "        num_tokens = batch_size * seq_len\n",
    "\n",
    "        # Reshape for routing\n",
    "        inp_reshaped = inp.reshape(\n",
    "            -1, self.input_dim\n",
    "        )  # [batch_size * seq_len, input_dim]\n",
    "\n",
    "        # Get router logits\n",
    "        router_logits = self.router(inp_reshaped)  # [batch_size * seq_len, num_experts]\n",
    "\n",
    "        # Add jitter noise during training for stability\n",
    "        if training and self.router_jitter > 0:\n",
    "            router_logits += torch.randn_like(router_logits) * self.router_jitter\n",
    "\n",
    "        # Calculate expert capacity: how many tokens can be routed to each expert\n",
    "        # We scale by capacity_factor to allow for some experts to receive more tokens\n",
    "        capacity = int(\n",
    "            self.capacity_factor * num_tokens * self.top_k / self.num_experts\n",
    "        )\n",
    "\n",
    "        # Convert router logits to probabilities using softmax\n",
    "        router_probs = F.softmax(\n",
    "            router_logits, dim=-1\n",
    "        )  # [batch_size * seq_len, num_experts]\n",
    "\n",
    "        # Get top-k experts and their routing probabilities\n",
    "        top_k_probs, top_k_indices = torch.topk(router_probs, self.top_k, dim=-1)\n",
    "\n",
    "        # Normalize the top-k probabilities\n",
    "        top_k_probs_sum = top_k_probs.sum(dim=-1, keepdim=True)\n",
    "        top_k_probs_normalized = top_k_probs / top_k_probs_sum\n",
    "\n",
    "        # Create mask for valid routing\n",
    "        # Each token routes to top_k experts\n",
    "        expert_mask = torch.zeros(\n",
    "            num_tokens, self.num_experts, device=router_logits.device, dtype=torch.bool\n",
    "        )\n",
    "\n",
    "        # Create indices for scatter operation\n",
    "        token_indices = (\n",
    "            torch.arange(num_tokens, device=router_logits.device)\n",
    "            .unsqueeze(1)\n",
    "            .expand(-1, self.top_k)\n",
    "        )\n",
    "\n",
    "        # Populate the expert mask\n",
    "        expert_mask.scatter_(1, top_k_indices, True)\n",
    "\n",
    "        # Calculate auxiliary load balancing loss\n",
    "        # We want to encourage all experts to be used equally\n",
    "        # 1. Compute the fraction of router probability assigned to each expert\n",
    "        router_prob_per_expert = router_probs.mean(0)\n",
    "\n",
    "        # 2. Compute auxiliary loss: minimize the variance in expert utilization\n",
    "        aux_loss = torch.mean(\n",
    "            self.num_experts * router_prob_per_expert * router_prob_per_expert\n",
    "        )\n",
    "\n",
    "        # Create dispatch and combine tensors\n",
    "        # These will be used to route inputs to experts and combine expert outputs\n",
    "\n",
    "        # Create dispatch mask tracking which tokens go to which experts with what weights\n",
    "        dispatch_tensor = torch.zeros(\n",
    "            num_tokens, self.num_experts, device=router_logits.device, dtype=self.dtype\n",
    "        )\n",
    "\n",
    "        # For each token and its top-k experts, set the corresponding weight\n",
    "        for token_idx in range(num_tokens):\n",
    "            for k in range(self.top_k):\n",
    "                expert_idx = top_k_indices[token_idx, k].item()\n",
    "                prob = top_k_probs_normalized[token_idx, k].item()\n",
    "                dispatch_tensor[token_idx, expert_idx] = prob\n",
    "\n",
    "        # The combine tensor is the same as the dispatch tensor in this implementation\n",
    "        # Some implementations might use different weights for combining\n",
    "        combine_tensor = dispatch_tensor.clone()\n",
    "\n",
    "        return {\n",
    "            \"dispatch_tensor\": dispatch_tensor,\n",
    "            \"combine_tensor\": combine_tensor,\n",
    "            \"router_logits\": router_logits,\n",
    "            \"router_probs\": router_probs,\n",
    "            \"aux_loss\": aux_loss,\n",
    "            \"top_k_indices\": top_k_indices,\n",
    "            \"top_k_probs\": top_k_probs_normalized,\n",
    "        }\n",
    "\n",
    "\n",
    "class Qwen2_5MoEDecoderLayer(torch.nn.Module):\n",
    "    def __init__(self, config: Qwen2_5MoEConfig, layer_idx: int):\n",
    "        super().__init__()\n",
    "        # In Qwen2_5MoEDecoderLayer.__init__\n",
    "        self.input_dim = config.hidden_size\n",
    "        self.output_dim = config.hidden_size\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.self_attn = Qwen2Attention(config=config, layer_idx=layer_idx)\n",
    "        # self.mlp = Qwen2MLP(config)\n",
    "        self.router = Qwen2_5MoEExpertRouter(\n",
    "            input_dim=config.hidden_size,\n",
    "            num_experts=config.num_experts,\n",
    "            top_k=config.top_k,\n",
    "            capacity_factor=config.capacity_factor,\n",
    "            aux_loss_weight=config.aux_loss_weight,\n",
    "            router_jitter=config.router_jitter,\n",
    "        )\n",
    "\n",
    "        # self.experts = nn.ModuleList([\n",
    "        #     nn.Sequential(\n",
    "        #         nn.Linear(input_dim, expert_dim, dtype=dtype),\n",
    "        #         nn.GELU(),\n",
    "        #         nn.Dropout(expert_dropout),\n",
    "        #         nn.Linear(expert_dim, output_dim, dtype=dtype)\n",
    "        #     )\n",
    "        #     for _ in range(num_experts)\n",
    "        # ])\n",
    "        self.experts = nn.ModuleList(\n",
    "            [\n",
    "                Qwen2MLP(\n",
    "                    config,\n",
    "                )\n",
    "                for _ in range(config.num_experts)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.input_layernorm = Qwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = Qwen2RMSNorm(\n",
    "            config.hidden_size, eps=config.rms_norm_eps\n",
    "        )\n",
    "        if config.sliding_window and config._attn_implementation != \"flash_attention_2\":\n",
    "            print(\n",
    "                f\"Sliding Window Attention is enabled but not implemented for `{config._attn_implementation}`; \"\n",
    "                \"unexpected results may be encountered.\"\n",
    "            )\n",
    "        # Output projection layer for better integration with the next layer\n",
    "        # self.output_proj = nn.Linear(output_dim, output_dim, dtype=dtype)\n",
    "\n",
    "    def forward(self, inp, training=True):\n",
    "        \"\"\"\n",
    "        Forward pass for the MoE layer\n",
    "\n",
    "        Args:\n",
    "            inp: Tensor of shape [batch_size, seq_len, input_dim] from attention layer\n",
    "            training: Whether the model is in training mode\n",
    "\n",
    "        Returns:\n",
    "            outputs: Tensor of shape [batch_size, seq_len, output_dim]\n",
    "            aux_loss: Load balancing auxiliary loss\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = inp.shape\n",
    "        num_tokens = batch_size * seq_len\n",
    "\n",
    "        # Get routing information\n",
    "        router_outputs = self.router(inp, training=training)\n",
    "        dispatch_tensor = router_outputs[\"dispatch_tensor\"]\n",
    "        combine_tensor = router_outputs[\"combine_tensor\"]\n",
    "        aux_loss = router_outputs[\"aux_loss\"]\n",
    "\n",
    "        # Reshape input for expert processing\n",
    "        inp_reshaped = inp.reshape(\n",
    "            -1, self.input_dim\n",
    "        )  # [batch_size * seq_len, input_dim]\n",
    "\n",
    "        # Initialize expert outputs\n",
    "        expert_outputs = torch.zeros(\n",
    "            num_tokens, self.output_dim, device=inp.device, dtype=inp.dtype\n",
    "        )\n",
    "\n",
    "        # Process each expert\n",
    "        for expert_idx, expert in enumerate(self.experts):\n",
    "            # Get tokens routed to this expert\n",
    "            expert_mask = dispatch_tensor[:, expert_idx] > 0\n",
    "            if not expert_mask.any():\n",
    "                continue\n",
    "\n",
    "            # Select tokens for this expert\n",
    "            expert_inputs = inp_reshaped[expert_mask]\n",
    "\n",
    "            # Get expert weights for these tokens\n",
    "            expert_weights = dispatch_tensor[expert_mask, expert_idx].unsqueeze(1)\n",
    "\n",
    "            # Process inputs with the expert\n",
    "            processed = expert(expert_inputs)\n",
    "\n",
    "            # Weight the outputs by router probabilities\n",
    "            weighted_outputs = processed * expert_weights\n",
    "\n",
    "            # Add to the total outputs\n",
    "            expert_outputs[expert_mask] += weighted_outputs\n",
    "\n",
    "        # Reshape back to original dimensions\n",
    "        merged_outputs = expert_outputs.reshape(batch_size, seq_len, self.output_dim)\n",
    "\n",
    "        # Apply final output projection to improve integration with normalization layer\n",
    "        outputs = self.input_layernorm(merged_outputs)\n",
    "        outputs = self.post_attention_layernorm(outputs)\n",
    "\n",
    "        return outputs, aux_loss\n",
    "\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "from transformers.modeling_outputs import ModelOutput\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MoEModelOutputWithPast(ModelOutput):\n",
    "    \"\"\"\n",
    "    Output class for MoE models.\n",
    "\n",
    "    Args:\n",
    "        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n",
    "            Sequence of hidden-states at the output of the last layer of the model.\n",
    "        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True`):\n",
    "            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n",
    "            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n",
    "            `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,\n",
    "            encoder_sequence_length, embed_size_per_head)`.\n",
    "            Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if\n",
    "            `config.is_encoder_decoder=True` in the cross-attention blocks) that can be used (see `past_key_values`\n",
    "            input) to speed up sequential decoding.\n",
    "        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`):\n",
    "            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n",
    "            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n",
    "            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n",
    "        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`):\n",
    "            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n",
    "            sequence_length)`.\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
    "            heads.\n",
    "\n",
    "        aux_loss (`torch.FloatTensor`, *optional*):\n",
    "            Auxiliary load balancing loss for the MoE layers.\n",
    "    \"\"\"\n",
    "\n",
    "    last_hidden_state: torch.FloatTensor\n",
    "    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    aux_loss: Optional[torch.FloatTensor] = None\n",
    "\n",
    "\n",
    "class Qwen2_5MoEModel(PreTrainedModel):\n",
    "    def __init__(self, config: Qwen2_5MoEConfig):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [Qwen2_5MoEDecoderLayer(config, i) for i in range(config.num_hidden_layers)]\n",
    "        )\n",
    "        self.norm = Qwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.rotary_emb = Qwen2RotaryEmbedding(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        position_ids=None,\n",
    "        past_key_values=None,\n",
    "        inputs_embeds=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Forward pass of the Qwen2_5MoEModel.\n",
    "\n",
    "        Args:\n",
    "            input_ids (torch.LongTensor): Indices of input sequence tokens (batch_size, seq_len)\n",
    "            attention_mask (torch.Tensor): Mask to avoid attention on padding tokens (batch_size, seq_len)\n",
    "            position_ids (torch.LongTensor): Indices of positions (batch_size, seq_len)\n",
    "            past_key_values (tuple): Cached past key and values for faster inference\n",
    "            inputs_embeds (torch.FloatTensor): Embedded inputs (batch_size, seq_len, hidden_size)\n",
    "            use_cache (bool): Whether to return a cache for faster inference\n",
    "            output_attentions (bool): Whether to return attention weights\n",
    "            output_hidden_states (bool): Whether to return all hidden states\n",
    "            return_dict (bool): Whether to return a ModelOutput or tuple\n",
    "\n",
    "        Returns:\n",
    "            BaseModelOutputWithPast or tuple: Model outputs\n",
    "        \"\"\"\n",
    "        output_attentions = (\n",
    "            output_attentions\n",
    "            if output_attentions is not None\n",
    "            else self.config.output_attentions\n",
    "        )\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states\n",
    "            if output_hidden_states is not None\n",
    "            else self.config.output_hidden_states\n",
    "        )\n",
    "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        return_dict = (\n",
    "            return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        )\n",
    "\n",
    "        # Create embedding for input tokens if not provided\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.embed_tokens(input_ids)\n",
    "\n",
    "        batch_size, seq_length = inputs_embeds.shape[:2]\n",
    "\n",
    "        # Create position ids if not provided\n",
    "        if position_ids is None:\n",
    "            position_ids = torch.arange(\n",
    "                0, seq_length, dtype=torch.long, device=inputs_embeds.device\n",
    "            ).unsqueeze(0)\n",
    "\n",
    "        # Compute rotary embeddings\n",
    "        cos, sin = self.rotary_emb(inputs_embeds, seq_length)\n",
    "\n",
    "        # Process through each decoder layer\n",
    "        hidden_states = inputs_embeds\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attns = () if output_attentions else None\n",
    "        next_decoder_cache = () if use_cache else None\n",
    "        all_router_logits = ()\n",
    "        total_aux_loss = 0.0\n",
    "\n",
    "        for idx, decoder_layer in enumerate(self.layers):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states += (hidden_states,)\n",
    "\n",
    "            past_key_value = (\n",
    "                past_key_values[idx] if past_key_values is not None else None\n",
    "            )\n",
    "\n",
    "            # Process through decoder layer\n",
    "            layer_outputs = decoder_layer(\n",
    "                hidden_states,\n",
    "                attention_mask=attention_mask,\n",
    "                position_ids=position_ids,\n",
    "                past_key_value=past_key_value,\n",
    "                output_attentions=output_attentions,\n",
    "                use_cache=use_cache,\n",
    "                cos=cos,\n",
    "                sin=sin,\n",
    "                training=self.training,\n",
    "            )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "            total_aux_loss += layer_outputs[1]\n",
    "\n",
    "            # Handle caching and outputs\n",
    "            if output_attentions:\n",
    "                all_self_attns += (layer_outputs[2],)\n",
    "\n",
    "            if use_cache:\n",
    "                next_decoder_cache += (layer_outputs[3],)\n",
    "\n",
    "        # Final layer norm\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states += (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(\n",
    "                v\n",
    "                for v in [\n",
    "                    hidden_states,\n",
    "                    next_decoder_cache,\n",
    "                    all_hidden_states,\n",
    "                    all_self_attns,\n",
    "                    total_aux_loss,\n",
    "                ]\n",
    "                if v is not None\n",
    "            )\n",
    "\n",
    "        return MoEModelOutputWithPast(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=next_decoder_cache,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attns,\n",
    "            # router_logits=all_router_logits,\n",
    "            aux_loss=total_aux_loss,\n",
    "        )\n",
    "\n",
    "\n",
    "class Qwen2_5MoePreTrainedModel(PreTrainedModel):\n",
    "    config_class = Qwen2_5MoEConfig\n",
    "    base_model_prefix = \"model\"\n",
    "    supports_gradient_checkpointing = True\n",
    "    _no_split_modules = [\"Qwen2_5MoEDecoderLayer\"]\n",
    "    _skip_keys_device_placement = \"past_key_values\"\n",
    "    _supports_flash_attn_2 = True\n",
    "    _supports_sdpa = True\n",
    "    _supports_cache_class = True\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        std = self.config.initializer_range\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=std)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "\n",
    "\n",
    "from transformers.generation import GenerationMixin\n",
    "\n",
    "\n",
    "class Qwen2_5MoEForCausalLM(Qwen2_5MoePreTrainedModel, GenerationMixin):\n",
    "    _tied_weights_keys = [\"lm_head.weight\"]\n",
    "    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n",
    "    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n",
    "\n",
    "    def __init__(self, config: Qwen2_5MoEConfig):\n",
    "        super().__init__(config)\n",
    "        self.model = Qwen2_5MoEModel(config)\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids, attention_mask=None, position_ids=None, head_mask=None\n",
    "    ):\n",
    "        outputs = self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "        )\n",
    "        hidden_states = outputs[0]\n",
    "        lm_logits = self.lm_head(hidden_states)\n",
    "        return lm_logits\n",
    "\n",
    "    def generate(\n",
    "        self, input_ids, attention_mask=None, position_ids=None, head_mask=None\n",
    "    ):\n",
    "        lm_logits = self(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "        )\n",
    "        # Implement your generation logic here\n",
    "        # For example, you can use beam search or sampling to generate text\n",
    "        # This is a placeholder for the actual generation logic\n",
    "        generated_text = torch.argmax(lm_logits, dim=-1)\n",
    "        return generated_text\n",
    "    \n",
    "    def merge_models(self, model_name_or_path: str):\n",
    "        \"\"\"\n",
    "        Merge the model with the specified model name or path.\n",
    "        This is a placeholder for the actual merging logic.\n",
    "        \"\"\"\n",
    "        # Implement your merging logic here\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:650: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:650: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Qwen2Config\n",
    "\n",
    "chat_model_config = Qwen2Config.from_pretrained(chat_model_name)\n",
    "moe_config = Qwen2_5MoEConfig(\n",
    "    vocab_size=chat_model_config.vocab_size,\n",
    "    hidden_size=chat_model_config.hidden_size,\n",
    "    intermediate_size=chat_model_config.intermediate_size,\n",
    "    num_hidden_layers=chat_model_config.num_hidden_layers,\n",
    "    num_attention_heads=chat_model_config.num_attention_heads,\n",
    "    num_experts=3,\n",
    "    top_k=1,\n",
    "    # capacity_factor=1.5,\n",
    "    # aux_loss_weight=0.01,\n",
    ")\n",
    "\n",
    "moe_model = Qwen2_5MoEForCausalLM(moe_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2_5MoEForCausalLM(\n",
       "  (model): Qwen2_5MoEModel(\n",
       "    (embed_tokens): Embedding(151936, 1536)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2_5MoEDecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k_proj): Linear(in_features=1536, out_features=2048, bias=True)\n",
       "          (v_proj): Linear(in_features=1536, out_features=2048, bias=True)\n",
       "          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "        )\n",
       "        (router): Qwen2_5MoEExpertRouter(\n",
       "          (router): Linear(in_features=1536, out_features=3, bias=False)\n",
       "        )\n",
       "        (experts): ModuleList(\n",
       "          (0-2): 3 x Qwen2MLP(\n",
       "            (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "            (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "            (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moe_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2Attention(\n",
      "    (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      ")\n",
      "Layer Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2Attention(\n",
      "    (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      ")\n",
      "Layer Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2Attention(\n",
      "    (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      ")\n",
      "Layer Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2Attention(\n",
      "    (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      ")\n",
      "Layer Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2Attention(\n",
      "    (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      ")\n",
      "Layer Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2Attention(\n",
      "    (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      ")\n",
      "Layer Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2Attention(\n",
      "    (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      ")\n",
      "Layer Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2Attention(\n",
      "    (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      ")\n",
      "Layer Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2Attention(\n",
      "    (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      ")\n",
      "Layer Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2Attention(\n",
      "    (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      ")\n",
      "Layer Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2Attention(\n",
      "    (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      ")\n",
      "Layer Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2Attention(\n",
      "    (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      ")\n",
      "Layer Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2Attention(\n",
      "    (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      ")\n",
      "Layer Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2Attention(\n",
      "    (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      ")\n",
      "Layer Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2Attention(\n",
      "    (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      ")\n",
      "Layer Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2Attention(\n",
      "    (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      ")\n",
      "Layer Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2Attention(\n",
      "    (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      ")\n",
      "Layer Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2Attention(\n",
      "    (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      ")\n",
      "Layer Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2Attention(\n",
      "    (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      ")\n",
      "Layer Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2Attention(\n",
      "    (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      ")\n",
      "Layer Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2Attention(\n",
      "    (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      ")\n",
      "Layer Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2Attention(\n",
      "    (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      ")\n",
      "Layer Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2Attention(\n",
      "    (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      ")\n",
      "Layer Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2Attention(\n",
      "    (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      ")\n",
      "Layer Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2Attention(\n",
      "    (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      ")\n",
      "Layer Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2Attention(\n",
      "    (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      ")\n",
      "Layer Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2Attention(\n",
      "    (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      ")\n",
      "Layer Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2Attention(\n",
      "    (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "for layer_idx in range(len(chat_model.model.layers)):\n",
    "    decoder_layer = chat_model.model.layers[layer_idx]\n",
    "    print(f\"Layer {decoder_layer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
